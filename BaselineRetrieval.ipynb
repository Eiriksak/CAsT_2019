{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will in this notebook evaluate the indri_ql_baseline run files for both training and testing. This is the description of the run:\n",
    "\n",
    "\n",
    "We provide an Indri baseline run with Query Likelihood run, including both the topics and run files. Queries are generated by running AllenNLP coreference resolution to perform rewriting and stopwords are removed using the Indri stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecEval, TrecRun, TrecQrel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File formats\n",
    "We need two files to evaluate\n",
    "#### Run file:\n",
    "\n",
    "FORMAT: qid Q0 docno rank score tag\n",
    "\n",
    "* qid\tis the query number\n",
    "* Q0\tis the literal Q0\n",
    "* docno\tis the id of a document returned for qid\n",
    "* rank\t(1-999) is the rank of this response for this qid\n",
    "* score\tis a system-dependent indication of the quality of the response\n",
    "* tag\tis the identifier for the system\n",
    "\n",
    "#### TrecQrel:\n",
    "\n",
    "FORMAT: qid 0 docno relevance\n",
    "\n",
    "* qid\tis the query number\n",
    "* 0\tis the literal 0\n",
    "* docno\tis the id of a document in your collection\n",
    "* relevance\tis how relevant is docno for qid\n",
    "\n",
    "### Explore run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"baseline/train_topics.teIn\"\n",
    "with open(run_path, 'r') as f:\n",
    "    run = []\n",
    "    for line in f:\n",
    "        run.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_1 Q0 MARCO_955948 1 -5.32579 indri',\n",
       " '1_1 Q0 MARCO_6203672 2 -5.40291 indri',\n",
       " '1_1 Q0 MARCO_5692406 3 -5.412 indri',\n",
       " '1_1 Q0 MARCO_849267 4 -5.4137 indri',\n",
       " '1_1 Q0 MARCO_2331424 5 -5.41859 indri',\n",
       " '1_1 Q0 MARCO_4455128 6 -5.4235 indri',\n",
       " '1_1 Q0 MARCO_8528286 7 -5.42714 indri',\n",
       " '1_1 Q0 MARCO_5780723 8 -5.42758 indri',\n",
       " '1_1 Q0 MARCO_920443 9 -5.44404 indri',\n",
       " '1_1 Q0 CAR_87772d4208721133d00d7d62f4eaaf164da5b4e3 10 -5.44505 indri']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore query file\n",
    "This is in Indri format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"treccastweb/2019/data/training/train_topics_mod.qrel\"\n",
    "with open(query_path, 'r') as f:\n",
    "    qrels = []\n",
    "    for line in f:\n",
    "        qrels.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_1 0 MARCO_955948 2',\n",
       " '1_1 0 MARCO_6203672 2',\n",
       " '1_1 0 MARCO_849267 0',\n",
       " '1_1 0 MARCO_2331424 0',\n",
       " '1_1 0 MARCO_4455128 0',\n",
       " '1_1 0 MARCO_5692406 1',\n",
       " '1_1 0 MARCO_8528286 0',\n",
       " '1_1 0 CAR_87772d4208721133d00d7d62f4eaaf164da5b4e3 0',\n",
       " '1_1 0 MARCO_920443 0',\n",
       " '1_1 0 MARCO_4903530 0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with TrecEval\n",
    "#### 1. Evaluate the given train run and qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = TrecRun(run_path)\n",
    "qrels = TrecQrel(query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TrecEval(run, qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_score = te.get_map(depth=10)\n",
    "mrr = te.get_reciprocal_rank(depth=10)\n",
    "ndcg = te.get_ndcg(depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.03701183744675955\n",
      "MRR: 0.07325927892842392\n",
      "NDCG@3: 0.043880365013731826\n"
     ]
    }
   ],
   "source": [
    "print('MAP: {}\\nMRR: {}\\nNDCG@3: {}'.format(map_score, mrr, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evaluate the given test run and qrel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores should be:\n",
    "* MAP: 0.139\n",
    "* MRR: 0.328\n",
    "* NDCG@3: 0.152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_path = \"baseline/test_topics.teIn\"\n",
    "test_qrels_path = \"2019qrels.txt\"\n",
    "run = TrecRun(test_run_path)\n",
    "qrels = TrecQrel(test_qrels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TrecEval(run, qrels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_score = te.get_map(per_query=True)\n",
    "mrr = te.get_reciprocal_rank(per_query=True)\n",
    "ndcg = te.get_ndcg(depth=3, per_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recip_rank@1000    0.407231\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAP@1000    0.129923\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NDCG@3    0.393203\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check per_query=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_score = te.get_map(per_query=False)\n",
    "mrr = te.get_reciprocal_rank(per_query=False)\n",
    "ndcg = te.get_ndcg(depth=3, per_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.04692423734583531\n",
      "MRR: 0.11477276954435729\n",
      "NDCG@3: 0.0533574446354449\n"
     ]
    }
   ],
   "source": [
    "print('MAP: {}\\nMRR: {}\\nNDCG@3: {}'.format(map_score, mrr, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from trec_eval repository\n",
    "go to the trec_eval repository and run:\n",
    "\n",
    "* For training: ./trec_eval ../treccastweb/2019/data/training/train_topics_mod.qrel ../baseline/train_topics.teIn\n",
    "* For test: ./trec_eval ../2019qrels.txt ../baseline/test_topics.teIn\n",
    "\n",
    "\n",
    "# TODO\n",
    "Find out why we get wrong score with both this python package and the original trec_eval program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
